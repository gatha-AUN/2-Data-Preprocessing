{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "# https://www.kaggle.com/shrutimechlearn/step-by-step-pca-with-iris-dataset\n",
    "\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing \n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Species']\n",
    "iris_dataset = read_csv(url, names = names)\n",
    "\n",
    "print (iris_dataset.head())\n",
    "\n",
    "# Since class names in Species column are of type string; Encoding it to integer values\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "iris_dataset['Species']= label_encoder.fit_transform(iris_dataset['Species']) \n",
    "\n",
    "iris_dataset['Species'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~ PCA as dimensionality reduction ~~~~~~~~~~~\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "Y = iris_dataset.Species\n",
    "X = iris_dataset.drop(['Species'],axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=20, stratify=Y)\n",
    "\n",
    "pca = PCA()\n",
    "X_new = pca.fit_transform(X)\n",
    "\n",
    "pca.get_covariance()\n",
    "\n",
    "# explained variance ratio by a principal component = \n",
    "# the ratio between the variance of that principal component and the total variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print (\"Explained Variance: \", explained_variance, \"\\n\")\n",
    "\n",
    "# How to choose number of components?\n",
    "pca = PCA().fit(X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "#\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plt.bar(range(4), explained_variance, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training KN classifier over reduced features \n",
    "pca = PCA(n_components = 3)\n",
    "X_new = pca.fit_transform(X)\n",
    "\n",
    "X_train_new, X_test_new, y_train, y_test = train_test_split(X_new, Y, test_size = 0.3, random_state=20, stratify=Y)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(7)\n",
    "knn_pca.fit(X_train_new,y_train)\n",
    "print(\"Train score after PCA: \", knn_pca.score(X_train_new, y_train),\"%\")\n",
    "print(\"Test score after PCA: \", knn_pca.score(X_test_new, y_test),\"%\")\n",
    "print (\"Explained Variance: \", pca.explained_variance_ratio_, \"\\n\")\n",
    "\n",
    "# Visualising the Test set results\n",
    "classifier = knn_pca\n",
    "X_set, y_set = X_test_new, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel(),np.zeros((X1.shape[0],X1.shape[1])).ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('pink', 'lightgreen')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('KNN PCA (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~ PCA for visualization ~~~~~~~~~~~\n",
    "\n",
    "# Load multidimensional digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n",
    "# Num features = 64 (= 8*8 pixels)\n",
    "\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(\"Original Shape: \", digits.data.shape)\n",
    "\n",
    "print(\"PCA Reduced Shape: \", projected.shape)\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c = digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap = plt.cm.Wistia) \n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using MNIST Digits dataset\n",
    "# How to choose number of components for Digits dataset?\n",
    "\n",
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~ PCA for noise filtering ~~~~~~~~~~~\n",
    "\n",
    "def plot_digits(data, descriptn):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    print (descriptn)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "        \n",
    "# Original Digits\n",
    "plot_digits(digits.data, \"Original Digits:\")\n",
    "\n",
    "# Adding random noise to create a noisy dataset\n",
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy, \"Noise Distorted Digits:\")\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(0.50).fit(noisy)\n",
    "print (\"Number of components for projection preserve of 50% of variance: \", pca.n_components_)\n",
    "\n",
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "\n",
    "plot_digits(filtered, \"PCA Re-constructed Digits:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
